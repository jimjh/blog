<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="https://blog.jimjh.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="https://blog.jimjh.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="https://blog.jimjh.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="James Lim">
  <meta name="description" content="Posts and writings by James Lim">

  <link href="http://feeds.feedburner.com/jimjh/blog" type="application/rss+xml" rel="alternate" title="Ampersand RSS" />

<meta name="keywords" content="distributed, devops">

  <title>
    Ampersand
&ndash; Building Elastic Clusters  </title>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-5604647-6']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>

<body>
  <aside>
    <div id="user_meta">
      <h2><a href="https://blog.jimjh.com">Ampersand</a></h2>
      <p>Moved to Medium in 2019.</p>
      <ul>
        <li><a href="https://medium.com/@jimjh" target="_blank">Medium</a></li>
        <li><a href="https://jimjh.com" target="_blank">Profile</a></li>
      </ul>
    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="https://blog.jimjh.com">Index</a> &brvbar; <a href="https://blog.jimjh.com/archives.html">Archives</a>
      &brvbar; <a href="http://feeds.feedburner.com/jimjh/blog">RSS</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="https://blog.jimjh.com/building-elastic-clusters.html">Building Elastic Clusters</a></h1>
  </div>
  <div class="article_meta">
    <p>Posted on: Mon 04 May 2015</p>
  </div>
  <div class="article_text">
    <p>(Authors: James Lim, <a href="http://addumb.com/">Adam Gray</a>)</p>
<p>One of the main advantages of using a cloud provider, such as Amazon Web
Services (AWS), is the ability to launch new instances on demand. However, it
also means giving up control over physical machines and their retirement
schedules; the cloud provider may terminate or retire a VM at any time for many
reasons. As a business grows, the probability that one of our many VMs
terminates on any given day increases, and it can rapidly become uneconomical
to manually administer these instances.</p>
<p>The answer to this problem is automation - distributed systems should recover
from failures and grow automatically.</p>
<p>At Quixey, we depend on distributed systems such as Zookeeper, Kafka, and
Cassandra in various critical parts of our infrastructure e.g. service
discovery, instrumentation, and storage for app-tier services. To ensure that
these foundational services can respond to traffic spikes and VM retirement
events, we have put in much effort into <a href="http://en.wikipedia.org/wiki/Elasticity_(data_store)#Clustering_elasticity">cluster elasticity</a>.</p>
<h2>Making Kafka Elastic</h2>
<p>Kafka is a distributed message system. Our Kafka clusters are configured with
default <a href="http://kafka.apache.org/documentation.html#replication">replication factor</a> of 3 and a fixed number of <a href="http://kafka.apache.org/documentation.html#introduction">partitions</a> for each
topic. In most cases, the clusters can tolerate up to 2 failures: at least one
of the 3 replicas has to remain available.</p>
<p>On AWS, Quixey uses <a href="http://aws.amazon.com/autoscaling/">Auto Scaling Groups</a> (ASG) to manage our Kafka clusters.
Each ASG is configured with a lower bound on the the number of instances, and a
scaling policy to increase the size of the cluster when necessary. When
instances are terminated, or when certain CloudWatch metrics (e.g. network I/O,
disk use) exceed their configured thresholds, new EC2 instances are launched.</p>
<p>When a new instance is launched, it requests for a new broker ID using a tool
called Bumper. Bumper increments an atomic integer on Zookeeper and copies it
to a file on disk for use as the broker ID. Our provisioning tool then installs
Kafka, starts the service, and runs a tool called Balancer. Balancer iterates
through a list of strategies to determine the partitions to reassign to the new
broker, such as under-replicated partitions. A different set of strategies can
be written for each cloud; on AWS, for example, we are concerned about
availability zone (AZ) outages, and want to ensure that each partition is
replicated over multiple AZs.</p>
<p>The above solution has been tested in production, and works reasonably well. We
are working on Balancer strategies to automatically increase the number of
partitions for heavy topics, so that their messages can be spread over more
instances.</p>
<h2>Making Zookeeper Elastic</h2>
<p>Zookeeper is a coordination service. It uses <a href="http://web.stanford.edu/class/cs347/reading/zab.pdf">ZAB</a> for replication, which
requires a quorum of servers to be up to remain available. For example, if we
have 3 nodes in the ensemble, we can afford to lose up to one of them.</p>
<p>Managing Zookeeper is slightly trickier than managing Kafka:  each Zookeeper
server needs to have the entire list of server IDs and address/port combos. Any
change to the set of members requires the administrator to update the
configuration files across the ensemble and perform a <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-107">rolling restart</a>. We tried
various solutions, and ended up using Netflix’s <a href="https://github.com/Netflix/exhibitor">Exhibitor</a>. Exhibitor is not as
mature as Zookeeper and has some rough edges, but we got it to do what we
wanted with a few slight modifications.</p>
<p>It works as follows: Exhibitor maintains a shared configuration on S3 or <a href="http://www.aliyun.com/product/oss/">OSS</a>,
and each instance in the cluster runs a pair of Exhibitor and Zookeeper
processes. Each Exhibitor process polls the shared configuration for changes,
and triggers a rolling restart when the list of members in the shared
configuration is updated.</p>
<p>In the event of an instance failure/termination, the relevant ASG launches a
replacement instance, which obtains a new identifier and adds itself to the
shared configuration. The Exhibitor processes will detect the change and
trigger a rolling restart across the ensemble to keep the configurations in
sync. Note that Exhibitor’s polling frequency is configurable.</p>
<p>Currently, the Zookeeper ASGs are configured to maintain the ensemble at a
fixed size; we intend to create policies that will increase the number of
<a href="http://zookeeper.apache.org/doc/r3.4.5/zookeeperObservers.html">observers</a> as the number of clients grows to improve read performance.</p>
<h2>Enforcing Discipline</h2>
<p>Modern Internet operations practices shrug off constraints on the uptime of any
specific single server by making all servers “trashable.” Working in the cloud
with on-demand instances allows us to terminate instances at the first sign of
trouble, such as <a href="http://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors">noisy neighbors</a> or hardware degradation. We can even do this
automatically when a server fails a health check for the service it is
providing (e.g. we cannot produce and consume a message in Kafka using that
server).</p>
<p>In an ideal deployment, humans should be paged only in extraordinary and
unexpected situations. Instance termination and traffic spikes are expected,
and should be handled by automation tools. This helps us focus more on
throughput and overall uptime and less on diagnosing individual problems. To
prevent future reoccurrences of issues, logs can be copied to S3/OSS for
post-mortem analysis of the system as a whole, rather than a case-by-case
examination of the nitty-gritty details.</p>
<p>As organizations mature and grow, they can no longer afford to handcraft and
tend to each <a href="https://blog.engineyard.com/2014/pets-vs-cattle">pet instance</a>. The realities of cloud services, such as VM
terminations and noisy neighbors, prod us to invest time into configuration
automation and make sure that every instance is <a href="http://martinfowler.com/bliki/PhoenixServer.html">reproducible</a>. Such discipline
can be enforced by terminating instances regularly and exercising the
automation scripts to prevent configuration drift, but that’s a topic for
another post.</p>
  </div>
  <div class="article_meta">
    <p>Category: <a href="https://blog.jimjh.com/category/software.html">Software</a>
 &ndash; Tags:
      <a href="https://blog.jimjh.com/tag/distributed.html">distributed</a>,      <a href="https://blog.jimjh.com/tag/devops.html">devops</a>    </p>
  </div>

  <div id="article_comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_identifier = "building-elastic-clusters.html";
        (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = '//jimjh.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
    </script>
  </div>

</article>


    <div id="ending_message">
      <p>&copy; James Lim. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>